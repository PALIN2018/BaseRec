# Transformer
n_layers: 1
n_heads: 1
hidden_size: 64
inner_size: 128
#hidden_dropout_prob: 0.5
#attn_dropout_prob: 0.5
hidden_act: 'gelu'
layer_norm_eps: 1e-12
initializer_range: 0.02
attention_constrain: 'uniform'
# CNN
embedding_size: 64
reg_weight: 1e-4
nv: 8
nh: 16
# GRU
num_layers: 1
# General
loss_type: 'CE'
dropout_prob: 0.4
# Pooling
last_k: 2